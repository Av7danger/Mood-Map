{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbda788",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Training (CPU-only)\n",
    "\n",
    "This notebook provides a clean implementation of sentiment analysis training using CPU only, avoiding any CUDA-related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99225578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%pip install pandas numpy torch scikit-learn joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce5865",
   "metadata": {},
   "source": [
    "## Force CPU Mode\n",
    "\n",
    "First, we'll force PyTorch to use CPU only to avoid any CUDA errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66149df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable CUDA usage completely\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "torch.cuda.is_available = lambda: False  # Override cuda availability check\n",
    "\n",
    "# Verify we're using CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b13c8",
   "metadata": {},
   "source": [
    "## Load & Preprocess Data\n",
    "\n",
    "We'll load the Twitter sentiment dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of the dataset\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # Load just a sample of the dataset for faster processing\n",
    "    chunk_size = 10000\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(pd.read_csv('backend/training.1600000.processed.noemoticon.csv', \n",
    "                                         encoding='latin-1', \n",
    "                                         header=None,\n",
    "                                         chunksize=chunk_size)):\n",
    "        if i >= 10:  # Take only 10 chunks = 100K samples\n",
    "            break\n",
    "        chunks.append(chunk)\n",
    "    data = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Assign column names\n",
    "    data.columns = ['label', 'id', 'date', 'query', 'user', 'text']\n",
    "    print(f\"Loaded {len(data)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataset\n",
    "print(\"Dataset overview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for class balance\n",
    "print(\"\\nClass distribution:\")\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075df58",
   "metadata": {},
   "source": [
    "## Feature Extraction with CountVectorizer\n",
    "\n",
    "We'll use sklearn's CountVectorizer for simpler and faster feature extraction instead of transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf72822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'], \n",
    "    data['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Map the Twitter sentiment labels (0=negative, 4=positive) to binary (0, 1)\n",
    "y_train = y_train.map({0: 0, 4: 1})\n",
    "y_test = y_test.map({0: 0, 4: 1})\n",
    "\n",
    "# Convert features using CountVectorizer\n",
    "print(\"Extracting features using CountVectorizer...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use a limited vocabulary size for efficiency\n",
    "max_features = 10000\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors (on CPU)\n",
    "X_train_tensor = torch.tensor(X_train_counts.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_counts.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Feature extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Training features shape: {X_train_tensor.shape}\")\n",
    "print(f\"Testing features shape: {X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f612e2",
   "metadata": {},
   "source": [
    "## Create PyTorch Datasets and DataLoaders\n",
    "\n",
    "We'll use PyTorch's Dataset and DataLoader for efficient batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0dd25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = SentimentDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64  # Small batch size for CPU\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1ef7d",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "We'll create a simple neural network for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, dropout_rate=0.2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create the model\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features\n",
    "model = SentimentClassifier(input_dim=input_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc29d6",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "Define a function to handle the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # End of epoch stats\n",
    "        epoch_end = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed in {epoch_end - epoch_start:.2f}s, Avg loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluate after each epoch\n",
    "        evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec16a49",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "Define a function to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b460b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f\"Test set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({accuracy*100:.2f}%)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109ab66",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now let's train the model using our defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    train(model, train_loader, criterion, optimizer, epochs=3)\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total training time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd212d07",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the trained model for later use in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86724f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and vectorizer\n",
    "try:\n",
    "    # Save PyTorch model\n",
    "    torch.save(model.state_dict(), 'sentiment_model.pt')\n",
    "    \n",
    "    # Save the vectorizer\n",
    "    joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "    \n",
    "    # Create a simple model wrapper for easy inference\n",
    "    class SentimentAnalysisModel:\n",
    "        def __init__(self, model, vectorizer):\n",
    "            self.model = model\n",
    "            self.vectorizer = vectorizer\n",
    "        \n",
    "        def predict(self, texts):\n",
    "            # Convert to list if single string\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            \n",
    "            # Transform texts to features\n",
    "            features = self.vectorizer.transform(texts)\n",
    "            features_tensor = torch.tensor(features.toarray(), dtype=torch.float32)\n",
    "            \n",
    "            # Get predictions\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(features_tensor)\n",
    "                _, predictions = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # Convert predictions to list\n",
    "            return predictions.numpy().tolist()\n",
    "    \n",
    "    # Create and save the complete model\n",
    "    complete_model = SentimentAnalysisModel(model, vectorizer)\n",
    "    joblib.dump(complete_model, 'model.pkl')\n",
    "    \n",
    "    print(\"Model saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91602bb6",
   "metadata": {},
   "source": [
    "## Test Model Inference\n",
    "\n",
    "Let's test the saved model with some example texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dff600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with some examples\n",
    "example_texts = [\n",
    "    \"I absolutely love this movie, it's amazing!\",\n",
    "    \"This product is terrible, I want my money back\",\n",
    "    \"The service was okay, nothing special\",\n",
    "    \"I'm having a great day today!\",\n",
    "    \"This is the worst experience I've ever had\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Load the complete model\n",
    "    loaded_model = joblib.load('model.pkl')\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = loaded_model.predict(example_texts)\n",
    "    \n",
    "    # Map predictions to sentiment labels\n",
    "    sentiment_labels = [\"Negative\" if pred == 0 else \"Positive\" for pred in predictions]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Example sentiment predictions:\")\n",
    "    for text, sentiment in zip(example_texts, sentiment_labels):\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Sentiment: {sentiment}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d4deb",
   "metadata": {},
   "source": [
    "## Integration with Flask Backend\n",
    "\n",
    "The model saved as `model.pkl` can be loaded by the Flask backend (app.py) to provide sentiment analysis services. The backend is already configured to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your sentiment analysis model is ready to be used by the Flask backend.\")\n",
    "print(\"The backend can now provide both sentiment analysis and text summarization services.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
