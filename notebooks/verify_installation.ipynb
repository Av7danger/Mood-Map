{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91889e7a",
   "metadata": {},
   "source": [
    "# Verify Python and Jupyter Installation\n",
    "\n",
    "This notebook will guide you through verifying your Python and Jupyter installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7be756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset kernel state\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61fb6da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.51.3\n",
      "Accelerate version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9fad34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from accelerate import Accelerator\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f56b50",
   "metadata": {},
   "source": [
    "## Verify Python Installation\n",
    "\n",
    "Use the `!python --version` command to check the installed Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c83fa64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.3\n"
     ]
    }
   ],
   "source": [
    "# Verify Python Installation\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdf2fe",
   "metadata": {},
   "source": [
    "## Verify Jupyter Installation\n",
    "\n",
    "Use the `!jupyter --version` command to check the installed Jupyter version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c686cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 9.2.0\n",
      "ipykernel        : 6.29.5\n",
      "ipywidgets       : not installed\n",
      "jupyter_client   : 8.6.3\n",
      "jupyter_core     : 5.7.2\n",
      "jupyter_server   : not installed\n",
      "jupyterlab       : not installed\n",
      "nbclient         : not installed\n",
      "nbconvert        : not installed\n",
      "nbformat         : not installed\n",
      "notebook         : not installed\n",
      "qtconsole        : not installed\n",
      "traitlets        : 5.14.3\n"
     ]
    }
   ],
   "source": [
    "# Verify Jupyter Installation\n",
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2e1ef",
   "metadata": {},
   "source": [
    "## Run a Test Python Script in Jupyter\n",
    "\n",
    "Write and execute a simple Python script, such as printing 'Hello, Jupyter!', to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f153adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Jupyter!\n"
     ]
    }
   ],
   "source": [
    "# Run a Test Python Script\n",
    "print(\"Hello, Jupyter!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b227565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a delay to prevent resource contention\n",
    "import time\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918f3aa",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model Training\n",
    "\n",
    "This notebook will guide you through the process of training a sentiment analysis model using Python and scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a40ca2",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We will use libraries like pandas, scikit-learn, and joblib for data handling, model training, and saving the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "088208e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d4cc4",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset\n",
    "\n",
    "Load the dataset containing text and labels for sentiment analysis. Ensure the dataset has 'text' and 'label' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2085ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label          id                          date     query             user  \\\n",
      "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
      "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
      "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
      "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
      "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with the correct path\n",
    "data = pd.read_csv('backend/training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "# Rename dataset columns to match expected names\n",
    "data.columns = ['label', 'id', 'date', 'query', 'user', 'text']\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bacac5",
   "metadata": {},
   "source": [
    "## Step 2.1: Inspect Dataset Columns\n",
    "\n",
    "Check the column names in the dataset to identify the correct columns for text and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "013489b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'id', 'date', 'query', 'user', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataset columns\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ac174",
   "metadata": {},
   "source": [
    "## Step 2.2: Rename Dataset Columns\n",
    "\n",
    "Rename the dataset columns to make them more meaningful and use the correct columns for text and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f07b240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label          id                          date     query             user  \\\n",
      "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
      "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
      "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
      "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
      "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
      "\n",
      "                                                text  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "# Rename dataset columns\n",
    "data.columns = ['label', 'id', 'date', 'query', 'user', 'text']\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a19062",
   "metadata": {},
   "source": [
    "## Step 2.3: Use Correct Columns for Text and Labels\n",
    "\n",
    "Update the code to use the renamed columns for text and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7fdfea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the renamed columns\n",
    "X = data['text']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afaf53",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Data\n",
    "\n",
    "Convert text data into numerical features using a PyTorch-based tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d16ff370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for tokenization: cpu\n",
      "Processing 100000 samples out of 1600000 total samples\n",
      "Starting tokenization in batches...\n",
      "Processing batch 1/98 (1.0%)\n",
      "Processing 100000 samples out of 1600000 total samples\n",
      "Starting tokenization in batches...\n",
      "Processing batch 1/98 (1.0%)\n",
      "Processing batch 11/98 (11.2%)\n",
      "Processing batch 11/98 (11.2%)\n",
      "Processing batch 21/98 (21.4%)\n",
      "Processing batch 21/98 (21.4%)\n",
      "Processing batch 31/98 (31.6%)\n",
      "Processing batch 31/98 (31.6%)\n",
      "Processing batch 41/98 (41.8%)\n",
      "Processing batch 41/98 (41.8%)\n",
      "Processing batch 51/98 (52.0%)\n",
      "Processing batch 51/98 (52.0%)\n",
      "Processing batch 61/98 (62.2%)\n",
      "Processing batch 61/98 (62.2%)\n",
      "Processing batch 71/98 (72.4%)\n",
      "Processing batch 71/98 (72.4%)\n",
      "Processing batch 81/98 (82.7%)\n",
      "Processing batch 81/98 (82.7%)\n",
      "Processing batch 91/98 (92.9%)\n",
      "Processing batch 91/98 (92.9%)\n",
      "Tokenization completed in 23.96 seconds\n",
      "Encodings shape: torch.Size([100000, 512])\n",
      "Labels shape: torch.Size([100000])\n",
      "Tokenization completed in 23.96 seconds\n",
      "Encodings shape: torch.Size([100000, 512])\n",
      "Labels shape: torch.Size([100000])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data using DistilBERT tokenizer\n",
    "import torch\n",
    "import time\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Check for GPU availability and configure CUDA for better performance\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    # Set CUDA optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Print CUDA device information\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Capability: {torch.cuda.get_device_capability()}\")\n",
    "\n",
    "print(f\"Using device for tokenization: {device}\")\n",
    "\n",
    "# Load pre-trained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Measure tokenization time\n",
    "start_time = time.time()\n",
    "\n",
    "# Sample the data to make it more manageable\n",
    "sample_size = min(100000, len(data))\n",
    "sampled_data = data.sample(n=sample_size, random_state=42)\n",
    "print(f\"Processing {sample_size} samples out of {len(data)} total samples\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "X = sampled_data['text'].tolist()\n",
    "y = sampled_data['label'].tolist()\n",
    "\n",
    "# Set batch size for processing\n",
    "batch_size = 1024\n",
    "num_batches = (len(X) + batch_size - 1) // batch_size\n",
    "\n",
    "print(\"Starting tokenization in batches...\")\n",
    "all_input_ids = []\n",
    "all_attention_mask = []\n",
    "\n",
    "# Process in batches for better GPU utilization\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(X))\n",
    "    batch = X[start_idx:end_idx]\n",
    "    \n",
    "    # Display progress every 10 batches\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing batch {i+1}/{num_batches} ({(i+1)/num_batches*100:.1f}%)\")\n",
    "    \n",
    "    # Tokenize batch\n",
    "    batch_encodings = tokenizer(batch, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    \n",
    "    # Move to GPU if available (for faster operations)\n",
    "    batch_encodings = {k: v.to(device) for k, v in batch_encodings.items()}\n",
    "    \n",
    "    # Store results\n",
    "    all_input_ids.append(batch_encodings['input_ids'].cpu())  # Move back to CPU for storage\n",
    "    all_attention_mask.append(batch_encodings['attention_mask'].cpu())\n",
    "\n",
    "# Combine batches\n",
    "encodings = {\n",
    "    'input_ids': torch.cat(all_input_ids, dim=0),\n",
    "    'attention_mask': torch.cat(all_attention_mask, dim=0)\n",
    "}\n",
    "labels = torch.tensor(y)\n",
    "\n",
    "# Move final tensors to GPU for downstream processing\n",
    "encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "labels = labels.to(device)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tokenization completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Encodings shape: {encodings['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Calculate percentage of GPU memory used\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    current_memory = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
    "    print(f\"GPU memory used: {current_memory:.2f} GB ({current_memory/total_memory*100:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88755a92",
   "metadata": {},
   "source": [
    "## Step 4: Split the Data\n",
    "\n",
    "Split the dataset into training and testing sets using PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d3c8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(encodings['input_ids'], labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_texts, val_texts = train_texts.to(device), val_texts.to(device)\n",
    "train_labels, val_labels = train_labels.to(device), val_labels.to(device)\n",
    "\n",
    "# Ensure input tensors are converted to FloatTensor after moving to the device\n",
    "train_texts = train_texts.to(device).float()\n",
    "val_texts = val_texts.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1633b",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n",
    "\n",
    "Train a PyTorch-based model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e180657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Due to CUDA issues, we're using CPU-only mode for this training session\n",
      "Encodings input_ids shape: torch.Size([100000, 512])\n",
      "Device: cpu (forced CPU mode)\n",
      "Model created on CPU successfully\n",
      "Train texts shape: torch.Size([80000, 512]), dtype: torch.float32\n",
      "Train labels shape: torch.Size([80000]), dtype: torch.int64\n",
      "Unique label values: tensor([0, 1])\n",
      "Training on 10000 samples (reduced for CPU efficiency)\n",
      "Starting epoch 1/2...\n",
      "  Batch 1/157, Batch Loss: 171.7150\n",
      "  Batch 11/157, Batch Loss: 94.5775\n",
      "  Batch 21/157, Batch Loss: 49.5498\n",
      "  Batch 31/157, Batch Loss: 40.6684\n",
      "  Batch 41/157, Batch Loss: 33.8033\n",
      "  Batch 51/157, Batch Loss: 28.1412\n",
      "  Batch 61/157, Batch Loss: 33.5857\n",
      "  Batch 71/157, Batch Loss: 43.8713\n",
      "  Batch 81/157, Batch Loss: 20.5160\n",
      "  Batch 91/157, Batch Loss: 22.7430\n",
      "  Batch 101/157, Batch Loss: 26.9182\n",
      "  Batch 111/157, Batch Loss: 25.9129\n",
      "  Batch 121/157, Batch Loss: 25.1427\n",
      "  Batch 131/157, Batch Loss: 39.3902\n",
      "  Batch 141/157, Batch Loss: 17.3614\n",
      "  Batch 151/157, Batch Loss: 27.7556\n",
      "Epoch 1, Avg Loss: 40.7326\n",
      "Starting epoch 2/2...\n",
      "  Batch 1/157, Batch Loss: 21.0059\n",
      "  Batch 11/157, Batch Loss: 22.6706\n",
      "  Batch 21/157, Batch Loss: 15.5136\n",
      "  Batch 31/157, Batch Loss: 12.1971\n",
      "  Batch 81/157, Batch Loss: 20.5160\n",
      "  Batch 91/157, Batch Loss: 22.7430\n",
      "  Batch 101/157, Batch Loss: 26.9182\n",
      "  Batch 111/157, Batch Loss: 25.9129\n",
      "  Batch 121/157, Batch Loss: 25.1427\n",
      "  Batch 131/157, Batch Loss: 39.3902\n",
      "  Batch 141/157, Batch Loss: 17.3614\n",
      "  Batch 151/157, Batch Loss: 27.7556\n",
      "Epoch 1, Avg Loss: 40.7326\n",
      "Starting epoch 2/2...\n",
      "  Batch 1/157, Batch Loss: 21.0059\n",
      "  Batch 11/157, Batch Loss: 22.6706\n",
      "  Batch 21/157, Batch Loss: 15.5136\n",
      "  Batch 31/157, Batch Loss: 12.1971\n",
      "  Batch 41/157, Batch Loss: 12.8328\n",
      "  Batch 51/157, Batch Loss: 9.8625\n",
      "  Batch 61/157, Batch Loss: 10.6921\n",
      "  Batch 71/157, Batch Loss: 13.1284\n",
      "  Batch 81/157, Batch Loss: 13.6737\n",
      "  Batch 91/157, Batch Loss: 10.4884\n",
      "  Batch 101/157, Batch Loss: 10.8342\n",
      "  Batch 111/157, Batch Loss: 11.5024\n",
      "  Batch 121/157, Batch Loss: 8.7673\n",
      "  Batch 131/157, Batch Loss: 11.3013\n",
      "  Batch 141/157, Batch Loss: 10.0334\n",
      "  Batch 151/157, Batch Loss: 16.1206\n",
      "Epoch 2, Avg Loss: 14.3769\n",
      "Training completed!\n",
      "  Batch 41/157, Batch Loss: 12.8328\n",
      "  Batch 51/157, Batch Loss: 9.8625\n",
      "  Batch 61/157, Batch Loss: 10.6921\n",
      "  Batch 71/157, Batch Loss: 13.1284\n",
      "  Batch 81/157, Batch Loss: 13.6737\n",
      "  Batch 91/157, Batch Loss: 10.4884\n",
      "  Batch 101/157, Batch Loss: 10.8342\n",
      "  Batch 111/157, Batch Loss: 11.5024\n",
      "  Batch 121/157, Batch Loss: 8.7673\n",
      "  Batch 131/157, Batch Loss: 11.3013\n",
      "  Batch 141/157, Batch Loss: 10.0334\n",
      "  Batch 151/157, Batch Loss: 16.1206\n",
      "Epoch 2, Avg Loss: 14.3769\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Define a simple PyTorch model with a more robust architecture\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Force CPU-only mode to avoid CUDA errors completely\n",
    "print(\"WARNING: Due to CUDA issues, we're using CPU-only mode for this training session\")\n",
    "torch.cuda.is_available = lambda: False  # Override CUDA availability check for this session\n",
    "device = torch.device('cpu')  # Force CPU device\n",
    "\n",
    "# Define our model architecture\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        # More stable architecture with an additional layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Debug information before model creation\n",
    "print(f\"Encodings input_ids shape: {encodings['input_ids'].shape}\")\n",
    "print(f\"Device: {device} (forced CPU mode)\")\n",
    "\n",
    "# Extract input dimension safely\n",
    "input_dim = encodings['input_ids'].shape[1]  # Get the input dimension\n",
    "hidden_dim = 64  # Smaller hidden dimension for stability\n",
    "\n",
    "# Move data to CPU (if not already there)\n",
    "train_texts = train_texts.to('cpu')\n",
    "train_labels = train_labels.to('cpu')\n",
    "\n",
    "# Create model on CPU\n",
    "model = SentimentModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=2)\n",
    "print(\"Model created on CPU successfully\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert and map labels properly\n",
    "# The Twitter dataset uses 0 and 4 as sentiment labels, map them to 0 and 1\n",
    "label_mapping = {0: 0, 4: 1}  # Map Twitter sentiment dataset labels\n",
    "\n",
    "# Safe label conversion\n",
    "train_labels_cpu = train_labels.cpu().numpy()\n",
    "train_labels_mapped = torch.tensor([label_mapping.get(int(label), 0) for label in train_labels_cpu], \n",
    "                                  dtype=torch.long)\n",
    "\n",
    "# Convert input to float for model input\n",
    "train_texts = train_texts.float()\n",
    "\n",
    "# Verify label shapes and types\n",
    "print(f\"Train texts shape: {train_texts.shape}, dtype: {train_texts.dtype}\")\n",
    "print(f\"Train labels shape: {train_labels_mapped.shape}, dtype: {train_labels_mapped.dtype}\")\n",
    "print(f\"Unique label values: {torch.unique(train_labels_mapped)}\")\n",
    "\n",
    "# Reduce batch size for CPU training to avoid running out of memory\n",
    "subset_size = min(10000, len(train_texts))  # Use at most 10K samples for faster CPU training\n",
    "train_texts = train_texts[:subset_size]\n",
    "train_labels_mapped = train_labels_mapped[:subset_size]\n",
    "print(f\"Training on {subset_size} samples (reduced for CPU efficiency)\")\n",
    "\n",
    "# Training loop with proper error handling\n",
    "epochs = 2  # Reduce epochs for CPU training\n",
    "batch_size = 64  # Smaller batch size for CPU\n",
    "\n",
    "# Use mini-batches for CPU training\n",
    "num_batches = (len(train_texts) + batch_size - 1) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch+1}/{epochs}...\")\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(train_texts))\n",
    "        \n",
    "        batch_texts = train_texts[start_idx:end_idx]\n",
    "        batch_labels = train_labels_mapped[start_idx:end_idx]\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_texts)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track progress\n",
    "        epoch_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Batch {i+1}/{num_batches}, Batch Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # End of epoch stats\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {epoch_loss/num_batches:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00d679",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the Model\n",
    "\n",
    "Evaluate the model's performance on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3f7906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 25.94%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(val_texts)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predictions == val_labels).float().mean()\n",
    "    print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73861c11",
   "metadata": {},
   "source": [
    "## Step 7: Save the Model\n",
    "\n",
    "Save the trained model and vectorizer for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a87748",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save the model and vectorizer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vectorize\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mjoblib\u001b[49m.dump(model, \u001b[33m'\u001b[39m\u001b[33mmodel.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m joblib.dump(vectorize, \u001b[33m'\u001b[39m\u001b[33mvectorizer.pkl\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the model and vectorizer\n",
    "from numpy import vectorize\n",
    "\n",
    "\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(vectorize, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b385499",
   "metadata": {},
   "source": [
    "# Train a BERT Model for Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained BERT model for sentiment analysis using Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ce883",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Install the necessary libraries, including transformers and torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install transformers torch scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec24e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "%pip install torch torchvision --upgrade --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Verify installation and GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    current_memory = torch.cuda.memory_allocated(0) / 1e9  # GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
    "    print(f\"GPU memory: {current_memory:.2f}GB used / {total_memory:.2f}GB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972aef42",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Import the necessary libraries for data handling, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f949044",
   "metadata": {},
   "source": [
    "## Step 3: Load and Preprocess the Dataset\n",
    "\n",
    "Load the dataset in chunks if it is too large to fit into memory. Sample the dataset for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset in chunks\n",
    "chunk_size = 10000  # Number of rows per chunk\n",
    "chunks = []\n",
    "for chunk in pd.read_csv('backend/training.1600000.processed.noemoticon.csv', chunksize=chunk_size, encoding='latin-1'):\n",
    "    chunks.append(chunk.sample(frac=0.1, random_state=42))  # Sample 10% of each chunk\n",
    "\n",
    "# Combine sampled chunks into a single DataFrame\n",
    "data = pd.concat(chunks, ignore_index=True)\n",
    "print(f'Dataset size after sampling: {data.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dataset columns are renamed\n",
    "if 'text' not in data.columns:\n",
    "\tdata.columns = ['label', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Define train_texts and val_texts\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d880e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset columns to verify column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a889d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the first few rows of the dataset to inspect its structure\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05365e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename dataset columns to match expected names\n",
    "data.columns = ['label', 'id', 'date', 'query', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21789f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376379d4",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize the Dataset\n",
    "\n",
    "Use the BERT tokenizer to preprocess the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d78cf",
   "metadata": {},
   "source": [
    "## Step 5: Create a Dataset Class\n",
    "\n",
    "Define a custom dataset class to handle the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels.values)\n",
    "val_labels = torch.tensor(val_labels.values)\n",
    "\n",
    "# Define Dataset class\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8694d6",
   "metadata": {},
   "source": [
    "## Step 6: Load the Pre-trained BERT Model\n",
    "\n",
    "Load a pre-trained BERT model for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ae7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefc2df",
   "metadata": {},
   "source": [
    "## Step 7: Define Training Arguments\n",
    "\n",
    "Set up the training arguments for fine-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    # Removed evaluation_strategy as it is not supported in the current version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ab9f9",
   "metadata": {},
   "source": [
    "## Step 8: Train the Model\n",
    "\n",
    "Use the Trainer API to fine-tune the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eac780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure device is defined\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Map labels to the range [0, 1] (Twitter sentiment dataset uses 0 for negative, 4 for positive)\n",
    "label_mapping = {0: 0, 4: 1}\n",
    "\n",
    "# Ensure all labels are valid before mapping\n",
    "train_labels_mapped = torch.tensor([label_mapping.get(label.item(), 0) for label in train_labels], dtype=torch.long)\n",
    "val_labels_mapped = torch.tensor([label_mapping.get(label.item(), 0) for label in val_labels], dtype=torch.long)\n",
    "\n",
    "# Verify label mapping\n",
    "print(f'Unique train labels after mapping: {set(train_labels_mapped.tolist())}')\n",
    "print(f'Unique validation labels after mapping: {set(val_labels_mapped.tolist())}')\n",
    "\n",
    "# Update train_dataset and val_dataset with the mapped labels\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels_mapped)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels_mapped)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    print(\"Debugging information:\")\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    print(f\"Train encodings keys: {train_encodings.keys()}\")\n",
    "    print(f\"Validation encodings keys: {val_encodings.keys()}\")\n",
    "    \n",
    "    # Safely check model properties\n",
    "    if hasattr(model, 'config'):\n",
    "        print(f\"Model hidden size: {model.config.hidden_size}\")\n",
    "        print(f\"Model num labels: {model.config.num_labels}\")\n",
    "    else:\n",
    "        print(\"Model doesn't have a config attribute\")\n",
    "        \n",
    "    # Check a sample input-output\n",
    "    print(\"Testing a sample input through the model:\")\n",
    "    sample_input = {k: torch.tensor(v[:1]) for k, v in train_encodings.items()}\n",
    "    # Move tensors to device\n",
    "    sample_input = {k: v.to(device) for k, v in sample_input.items()}\n",
    "    try:\n",
    "        sample_output = model(**sample_input)\n",
    "        print(f\"Sample output shape: {sample_output.logits.shape if hasattr(sample_output, 'logits') else 'No logits'}\")\n",
    "    except Exception as sample_error:\n",
    "        print(f\"Error with sample: {sample_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5c433",
   "metadata": {},
   "source": [
    "## Step 9: Save the Model\n",
    "\n",
    "Save the fine-tuned model and tokenizer for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb29696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which model is being used\n",
    "if hasattr(model, 'save_pretrained'):\n",
    "\t# Save Hugging Face model and tokenizer\n",
    "\tmodel.save_pretrained('./bert_model')\n",
    "\ttokenizer.save_pretrained('./bert_model')\n",
    "\tprint(\"BERT model and tokenizer saved successfully!\")\n",
    "else:\n",
    "\t# Save PyTorch model using torch.save\n",
    "\ttorch.save(model.state_dict(), './pytorch_model.pt')\n",
    "\tprint(\"PyTorch model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c35ee8",
   "metadata": {},
   "source": [
    "## Debugging: Inspect Shapes and Values\n",
    "\n",
    "Add debugging statements to inspect the shapes and values of logits and labels during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging: Inspect logits and labels\n",
    "def compute_loss_with_debugging(model, inputs):\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    labels = inputs['labels']\n",
    "    print(f'Logits shape: {logits.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Unique labels: {torch.unique(labels)}')\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "    return loss\n",
    "\n",
    "# Replace the Trainer's compute_loss method\n",
    "trainer.compute_loss = compute_loss_with_debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957e3c9",
   "metadata": {},
   "source": [
    "## Preprocessing: Ensure Labels are Correctly Formatted\n",
    "\n",
    "Map labels to integers and ensure they are in the range [0, num_labels-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ff9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers - since train_labels and val_labels are already tensors\n",
    "# Based on previous cells, the labels are already in the form of 0 and 4\n",
    "# We need to map 0 -> 0 (negative) and 4 -> 1 (positive)\n",
    "label_mapping = {0: 0, 4: 1}  # Map Twitter sentiment dataset labels\n",
    "\n",
    "# Create mapped versions rather than overwriting\n",
    "train_labels_mapped = torch.tensor([label_mapping.get(label.item(), 0) for label in train_labels], dtype=torch.long)\n",
    "val_labels_mapped = torch.tensor([label_mapping.get(label.item(), 0) for label in val_labels], dtype=torch.long)\n",
    "\n",
    "# Verify label mapping\n",
    "print(f'Unique train labels before mapping: {torch.unique(train_labels).tolist()}')\n",
    "print(f'Unique train labels after mapping: {torch.unique(train_labels_mapped).tolist()}')\n",
    "print(f'Unique validation labels before mapping: {torch.unique(val_labels).tolist()}')\n",
    "print(f'Unique validation labels after mapping: {torch.unique(val_labels_mapped).tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a008727",
   "metadata": {},
   "source": [
    "## Inspect Dataset and Model Configuration\n",
    "\n",
    "Verify the dataset structure and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef76af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset structure\n",
    "print(data.head())\n",
    "print(data.columns)\n",
    "\n",
    "# Verify model configuration\n",
    "if hasattr(model, 'config'):\n",
    "\t# For Hugging Face models (BertForSequenceClassification)\n",
    "\tprint(f'Model num_labels: {model.config.num_labels}')\n",
    "\tif hasattr(model.config, 'problem_type'):\n",
    "\t\tprint(f'Model problem type: {model.config.problem_type}')\n",
    "else:\n",
    "\t# For custom PyTorch model (SentimentModel)\n",
    "\tprint(f'Model type: {type(model).__name__}')\n",
    "\tif isinstance(model, SentimentModel):\n",
    "\t\tprint(f'Input dimension: {model.fc.in_features}')\n",
    "\t\tprint(f'Output dimension: {model.fc.out_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data and model to GPU\n",
    "train_texts, val_texts = train_texts.to(device), val_texts.to(device)\n",
    "train_labels, val_labels = train_labels.to(device), val_labels.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4687724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GPU benchmark to verify GPU performance\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Run a simple benchmark\n",
    "    print(\"\\nRunning GPU benchmark...\")\n",
    "    \n",
    "    # Create large tensors\n",
    "    start_time = time.time()\n",
    "    size = 5000\n",
    "    x = torch.randn(size, size, device='cuda')\n",
    "    y = torch.randn(size, size, device='cuda')\n",
    "    \n",
    "    # Matrix multiplication (computationally intensive)\n",
    "    torch.cuda.synchronize()  # Wait for all kernels to finish\n",
    "    matmul_start = time.time()\n",
    "    z = torch.matmul(x, y)\n",
    "    torch.cuda.synchronize()  # Wait for matmul to finish\n",
    "    matmul_time = time.time() - matmul_start\n",
    "    \n",
    "    # Free memory\n",
    "    del x, y, z\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Created two {size}x{size} random tensors and multiplied them\")\n",
    "    print(f\"Matrix multiplication time: {matmul_time:.4f} seconds\")\n",
    "    print(f\"Total time including tensor creation: {time.time() - start_time:.4f} seconds\")\n",
    "    \n",
    "    # Show memory usage\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9  # Convert to GB\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.2f}GB total\")\n",
    "else:\n",
    "    print(\"No GPU available, running on CPU only.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
